import os
import streamlit as st
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
# è®¾ç½®API
os.environ["OPENAI_API_KEY"] = "sk-5e030cc687b846718c190775c9ac6064"
os.environ["OPENAI_API_BASE"] = "https://api.deepseek.com"

DATA_DIR = "data/contracts"
VECTOR_DIR = "vectorstores/contracts"

st.set_page_config(
    page_title="çŸ¥è¯†åº“æµ‹è¯•Demo",
    layout="wide"
)

# ================= å·¥å…·å‡½æ•° =================

@st.cache_resource
def load_embeddings():
    return HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True},
    )


def load_documents():
    documents = []
    if not os.path.exists(DATA_DIR):
        os.makedirs(DATA_DIR)
        return documents

    for file in os.listdir(DATA_DIR):
        path = os.path.join(DATA_DIR, file)
        try:
            if file.lower().endswith(".pdf"):
                loader = PyPDFLoader(path)
                docs = loader.load()
            elif file.lower().endswith(".txt"):
                loader = TextLoader(path, encoding="utf-8")
                docs = loader.load()
            else:
                continue

            for d in docs:
                d.metadata["source"] = file
                documents.append(d)
        except Exception as e:
            st.error(f"åŠ è½½ {file} å¤±è´¥ï¼š{e}")

    return documents


def build_vectorstore():
    docs = load_documents()
    if not docs:
        return False, "æœªæ‰¾åˆ°æ–‡ä»¶"

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=600,
        chunk_overlap=80,
        separators=[
            "\n\n\n",
            "\n\n",
            "\n",
            "ã€‚"
        ]
    )

    split_docs = splitter.split_documents(docs)

    embeddings = load_embeddings()
    vectorstore = FAISS.from_documents(split_docs, embeddings)
    os.makedirs(os.path.dirname(VECTOR_DIR), exist_ok=True)
    vectorstore.save_local(VECTOR_DIR)

    return True, f"çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼ˆ{len(split_docs)} ä¸ªç‰‡æ®µï¼‰"


def ask_llm(question: str):
    embeddings = load_embeddings()
    vectorstore = FAISS.load_local(
        VECTOR_DIR,
        embeddings,
        allow_dangerous_deserialization=True
    )

    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    docs = retriever.invoke(question)

    if not docs:
        return "æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚", []

    context = "\n\n".join(
        f"æ¥æºï¼š{d.metadata.get('source')}\nå†…å®¹ï¼š{d.page_content}"
        for d in docs
    )

    llm = ChatOpenAI(
        model="deepseek-chat",
        temperature=0.3,
        max_tokens=1500
    )

    prompt = ChatPromptTemplate.from_template(
        """ä½ æ˜¯XXå…¬å¸çš„ä¼ä¸šçŸ¥è¯†åº“æ™ºèƒ½åŠ©ç†ï¼Œ
    ä¸»è¦èŒè´£æ˜¯åŸºäºå…¬å¸å†…éƒ¨èµ„æ–™ï¼Œä¸ºå®¢æˆ·å’Œå‘˜å·¥æä¾›å‡†ç¡®ã€ä¸“ä¸šçš„å›ç­”ã€‚

    è¯·ä¸¥æ ¼ä¾æ®ç»™å®šçš„å…¬å¸èµ„æ–™å†…å®¹è¿›è¡Œå›ç­”ã€‚
    å¦‚æœèµ„æ–™ä¸­æ²¡æœ‰æ˜ç¡®è¯´æ˜ï¼Œè¯·å›ç­”â€œèµ„æ–™ä¸­æœªæ˜ç¡®è¯´æ˜â€ã€‚

    ã€å…¬å¸èµ„æ–™ã€‘
    {context}

    ã€é—®é¢˜ã€‘
    {question}

    ã€å›ç­”è¦æ±‚ã€‘
    - åªåŸºäºèµ„æ–™å†…å®¹å›ç­”ï¼Œä¸è¦æ¨æµ‹æˆ–ç¼–é€ 
    - ä½¿ç”¨ä¸“ä¸šã€æ¸…æ™°ã€é€‚åˆå•†åŠ¡æ²Ÿé€šçš„ä¸­æ–‡
    - æ¡ç†æ¸…æ¥šï¼Œå¯ä½¿ç”¨åˆ†ç‚¹è¯´æ˜
    - ä¸è¦å‡ºç°â€œæ ¹æ®æˆ‘çš„ç†è§£â€â€œå¯èƒ½æ˜¯â€ç­‰ä¸ç¡®å®šè¡¨è¿°
    """
    )

    chain = prompt | llm
    answer = chain.invoke({
        "context": context,
        "question": question
    })

    return answer.content.strip(), docs


# ================= é¡µé¢ =================

st.title("ğŸ“„ ä¼ä¸šçŸ¥è¯†åº“æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")

# with st.sidebar:
#     st.header("ğŸ“‚ çŸ¥è¯†åº“ç®¡ç†")

#     if st.button("ğŸ”„ æ„å»º / æ›´æ–°çŸ¥è¯†åº“"):
#         with st.spinner("æ­£åœ¨æ„å»ºä¼ä¸šçŸ¥è¯†åº“..."):
#             ok, msg = build_vectorstore()
#         if ok:
#             st.success(msg)
#         else:
#             st.warning(msg)

#     st.markdown("---")
#     st.markdown(
#         """
#         **ä½¿ç”¨è¯´æ˜**
#         1. å°†å…¬å¸èµ„æ–™ï¼ˆäº§å“è¯´æ˜ / å·¥ç¨‹æ¡ˆä¾‹ / åˆåŒæ¡æ¬¾ç­‰ï¼‰æ”¾å…¥ `data/contracts`
#         2. ç‚¹å‡»ã€Œæ„å»º / æ›´æ–°çŸ¥è¯†åº“ã€
#         3. åœ¨å³ä¾§è¾“å…¥é—®é¢˜è¿›è¡ŒæŸ¥è¯¢
#         """
#     )

question = st.text_input(
    "è¯·è¾“å…¥æ‚¨æƒ³äº†è§£çš„é—®é¢˜ï¼š",
    placeholder="ä¾‹å¦‚ï¼šå•†åœºé¡¹ç›®ä¸€èˆ¬æ¨èä½¿ç”¨å“ªäº›ç¯å…·ï¼Ÿ"
)

if st.button("ğŸ” æ™ºèƒ½æŸ¥è¯¢") and question:
    if not os.path.exists(VECTOR_DIR):
        st.warning("è¯·å…ˆæ„å»ºä¼ä¸šçŸ¥è¯†åº“")
    else:
        with st.spinner("æ­£åœ¨åˆ†æä¼ä¸šçŸ¥è¯†åº“å†…å®¹..."):
            answer, docs = ask_llm(question)

        st.subheader("âœ… æ™ºèƒ½è§£ç­”")
        st.write(answer)

        with st.expander("ğŸ“„ æŸ¥çœ‹å‚è€ƒçš„å…¬å¸èµ„æ–™åŸæ–‡"):
            for i, d in enumerate(docs, 1):
                st.markdown(f"**æ®µè½ {i}ï½œæ¥æºï¼š{d.metadata.get('source')}**")
                st.write(d.page_content)
                st.markdown("---")
